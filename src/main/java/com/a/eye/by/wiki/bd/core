一、Spark运行架构
1、用户通过IDE编写程序-->打成jar包-->上传到集群-->开始运行

二、在Spark提交应用程序的机器执行的操作过程
1、Shell中spark-submit脚本提交程序
2、通过Standalone模式，其实会通过反射的方式创建和构造一个DriverActor进程。
3、创建SparkContext对象。
SparkContext在初始化时，做的最重要的时期是构造一个DAGScheduler和一个TaskScheduler
4、TaskScheduler实际上是会负责通过它对应的一个后台进行，去连接master向Master注册这个Application。

三、在Spark集群中
1、Master接受到Application注册过来的请求后，会用资源调度的算法，在Spark集群Worker上为这个App启动多个Executor。
2、Executor会创建线程池，每个线程会运行一个TaskRunner
3、Executor启动后会反向注册到TaskScheduler上面来。
4、所有的Executor都反向注册到Driver上之后，Driver结束SparkContext初始化，会继续执行我们自己编写的代码。
5、TaskScheduler会把TaskSet里面的每一个Task提交到右边的Executor上去执行。
6、最后整个这个Spark应用程序执行就是Stage分批次的做为TaskSet提交到Executor里面执行，每个Task针对RDD的一个partition，执行我们的
算子和函数，RDD可以有5亿或者10亿的数据，Task并行执行。








